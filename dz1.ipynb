{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLI_A_lpPNc1"
   },
   "source": [
    "# **Spark Apache (Семинары)**\n",
    "## **Урок 1. SQL & BigData**\n",
    "### Задание:\n",
    "    1. Условие: найти самую длинную последовательность упорядоченных чисел в RDD и вывести ее"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nFmGeP6PrF6"
   },
   "source": [
    "## Решение: [тут](#dz1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O76c5A_KOS5t"
   },
   "source": [
    "## Семинар"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 11137,
     "status": "ok",
     "timestamp": 1712060364999,
     "user": {
      "displayName": "Дмитрий Морданов",
      "userId": "02885086122102025996"
     },
     "user_tz": -180
    },
    "id": "VVEuzJXHPuUw",
    "outputId": "130063a3-bac4-4e2f-dae6-5262fdbadf17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7 (from pyspark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=175c8c358ffadf9b3d27a584f7f5972de51326406536a0f0469bbecb79772bea\n",
      "  Stored in directory: /home/dima/.cache/pip/wheels/95/13/41/f7f135ee114175605fb4f0a89e7389f3742aa6c1e1a5bcb657\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.1\n"
     ]
    }
   ],
   "source": [
    "! pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1712060364999,
     "user": {
      "displayName": "Дмитрий Морданов",
      "userId": "02885086122102025996"
     },
     "user_tz": -180
    },
    "id": "bvU_zgZcQBCj"
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "#%env PYSPARK_ALLOW_INSECURE_GATEWAY=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6PSAAPzRCCl"
   },
   "source": [
    "### Задание 1. Найти среднее значение элеменов RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 16020,
     "status": "ok",
     "timestamp": 1712060381013,
     "user": {
      "displayName": "Дмитрий Морданов",
      "userId": "02885086122102025996"
     },
     "user_tz": -180
    },
    "id": "KZ6aGgRiQXmG",
    "outputId": "f1f1af8b-716d-4b04-bd1d-5b77a004bd1e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/04 00:10:48 WARN Utils: Your hostname, zep-ntb resolves to a loopback address: 127.0.1.1; using 192.168.1.38 instead (on interface wlp5s0)\n",
      "24/04/04 00:10:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/04 00:10:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/04/04 00:11:07 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)/ 1]\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\n",
      "\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n",
      "\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:66)\n",
      "\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1282)\n",
      "\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1276)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1019)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/04/04 00:11:07 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (192.168.1.38 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\n",
      "\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n",
      "\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:66)\n",
      "\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1282)\n",
      "\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1276)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1019)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/04/04 00:11:07 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (192.168.1.38 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\n\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:66)\n\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1282)\n\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1276)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1019)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.immutable.List.foreach(List.scala:333)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\n\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:66)\n\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1282)\n\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1276)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1019)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage RDD\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m rdd \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mparallelize([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m mean_value \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mСреднее значение элементов в RDD:\u001b[39m\u001b[38;5;124m\"\u001b[39m, mean_value)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/pyspark/rdd.py:2523\u001b[0m, in \u001b[0;36mRDD.mean\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2501\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[NumberOrArray]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m   2502\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2503\u001b[0m \u001b[38;5;124;03m    Compute the mean of this RDD's elements.\u001b[39;00m\n\u001b[1;32m   2504\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;124;03m    2.0\u001b[39;00m\n\u001b[1;32m   2522\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats()\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/pyspark/rdd.py:2343\u001b[0m, in \u001b[0;36mRDD.stats\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2340\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mredFunc\u001b[39m(left_counter: StatCounter, right_counter: StatCounter) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m StatCounter:\n\u001b[1;32m   2341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m left_counter\u001b[38;5;241m.\u001b[39mmergeStats(right_counter)\n\u001b[0;32m-> 2343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmapPartitions(\u001b[38;5;28;01mlambda\u001b[39;00m i: [StatCounter(i)])\u001b[38;5;241m.\u001b[39mreduce(  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   2344\u001b[0m     redFunc\n\u001b[1;32m   2345\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/pyspark/rdd.py:1924\u001b[0m, in \u001b[0;36mRDD.reduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m reduce(f, iterator, initial)\n\u001b[0;32m-> 1924\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmapPartitions(func)\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vals:\n\u001b[1;32m   1926\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reduce(f, vals)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/pyspark/rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD\u001b[38;5;241m.\u001b[39mcollectAndServe(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd\u001b[38;5;241m.\u001b[39mrdd())\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (192.168.1.38 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\n\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:66)\n\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1282)\n\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1276)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1019)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.immutable.List.foreach(List.scala:333)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\n\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:66)\n\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1282)\n\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1276)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1019)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(\"local\", \"Average RDD\")\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "mean_value = rdd.mean()\n",
    "print(\"Среднее значение элементов в RDD:\", mean_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1712060381014,
     "user": {
      "displayName": "Дмитрий Морданов",
      "userId": "02885086122102025996"
     },
     "user_tz": -180
    },
    "id": "jDOG_s-_Q8TQ",
    "outputId": "c3b4e3b6-c56e-4a40-bdf1-7b3ac56d74ef"
   },
   "outputs": [],
   "source": [
    "active_session = SparkSession.active\n",
    "print(active_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1419,
     "status": "ok",
     "timestamp": 1712060382423,
     "user": {
      "displayName": "Дмитрий Морданов",
      "userId": "02885086122102025996"
     },
     "user_tz": -180
    },
    "id": "hlBfQiGWRXUw"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HO8OPjgORc9S"
   },
   "source": [
    "### Задание 2. Найти наибольший элемент RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 1908,
     "status": "ok",
     "timestamp": 1712060384328,
     "user": {
      "displayName": "Дмитрий Морданов",
      "userId": "02885086122102025996"
     },
     "user_tz": -180
    },
    "id": "FFhP7-4JRa8g",
    "outputId": "df3a49fd-1ad9-4bb9-e92e-ee0924c25b35"
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\", \"Max RDD\")\n",
    "rdd = sc.parallelize([100, 25, 30, 40, 55, 70])\n",
    "max_value = rdd.max()\n",
    "print(\"Наибольший элемент в RDD:\", max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 536,
     "status": "ok",
     "timestamp": 1712060384861,
     "user": {
      "displayName": "Дмитрий Морданов",
      "userId": "02885086122102025996"
     },
     "user_tz": -180
    },
    "id": "trL7Xm6HSBzA"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nu6dFIeFSJoK"
   },
   "source": [
    "### Задание 3. Подсчитать количество элементов, удовлетворяющих определенному условию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 1594,
     "status": "ok",
     "timestamp": 1712060386452,
     "user": {
      "displayName": "Дмитрий Морданов",
      "userId": "02885086122102025996"
     },
     "user_tz": -180
    },
    "id": "Igzb-UYiSI-S",
    "outputId": "9002cc1d-1387-4276-cc6a-891c1f768535"
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\", \"Filter RDD\")\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "filter_rdd = rdd.filter(lambda x: x > 5)\n",
    "count = filter_rdd.count()\n",
    "print(\"Количество элементов, больше 5:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 893,
     "status": "ok",
     "timestamp": 1712060387341,
     "user": {
      "displayName": "Дмитрий Морданов",
      "userId": "02885086122102025996"
     },
     "user_tz": -180
    },
    "id": "vTheUghBTZSl"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "has7eoJ8SWlJ"
   },
   "source": [
    "### Задание 4. Задание на группировку по ключу\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FG5qrPsxWuIS"
   },
   "source": [
    "Дан набор данных с информацией о продажах товаров в магазине в следующем формате: (товар, магазин, количество). Необходимо сгруппировать данные по по товару и найти суммарное количество проданных товаров по каждому товару."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 6613,
     "status": "ok",
     "timestamp": 1712060393951,
     "user": {
      "displayName": "Дмитрий Морданов",
      "userId": "02885086122102025996"
     },
     "user_tz": -180
    },
    "id": "7NvUf6iHTnya",
    "outputId": "4d707b08-b89c-49fe-9034-1a0c779db1fd"
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\", \"advanced1\")\n",
    "data = [(\"apple\", \"store1\", 10), (\"apple\", \"store2\", 15),\n",
    "        (\"banana\", \"store1\", 20), (\"banana\", \"store2\", 25),\n",
    "        (\"peach\", \"store1\", 5), (\"peach\", \"store2\", 10),\n",
    "        (\"peach\", \"store3\", 25),]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "grouped_rdd = rdd.map(lambda x: (x[0], x[2])).reduceByKey(lambda a, b: a + b)\n",
    "grouped_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1003,
     "status": "ok",
     "timestamp": 1712060394949,
     "user": {
      "displayName": "Дмитрий Морданов",
      "userId": "02885086122102025996"
     },
     "user_tz": -180
    },
    "id": "KbK-_NuWV7pR"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcDlUG_BSdS6"
   },
   "source": [
    "### Задание 5. Задание на аггрегацию по ключу\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abdMsuhvWzwn"
   },
   "source": [
    "Дан набор данных с информацией о продажах товаров в магазине в следующем формате: (магазин, товар, количество, цена). Необходимо найти общую выручку от продаж каждого товара в каждом магазине"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 2291,
     "status": "ok",
     "timestamp": 1712060397237,
     "user": {
      "displayName": "Дмитрий Морданов",
      "userId": "02885086122102025996"
     },
     "user_tz": -180
    },
    "id": "YiT4k1a9V6Sw",
    "outputId": "6128b2b0-5189-4f62-de3b-c5889fbce9da"
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\", \"advanced2\")\n",
    "data = [(\"store1\", \"apple\", 10, 2), (\"store2\", \"apple\", 15, 2.5),\n",
    "        (\"store1\", \"banana\", 20, 1.5), (\"store2\", \"banana\", 25, 1.8),\n",
    "        (\"store1\", \"watermelon\", 3, 5), (\"store2\", \"watermelon\", 2, 4),]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "revenue_rdd = rdd.map(lambda x: ((x[0], x[1]), x[2]*x[3])).reduceByKey(lambda a, b: a + b)\n",
    "revenue_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 981,
     "status": "ok",
     "timestamp": 1712060398215,
     "user": {
      "displayName": "Дмитрий Морданов",
      "userId": "02885086122102025996"
     },
     "user_tz": -180
    },
    "id": "3IJ1cnU1XS0u"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfvY89yQcPz3"
   },
   "source": [
    "### Задание 6. Задание на JOIN по ключу\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWuNZ88JcUAZ"
   },
   "source": [
    "Даны два набора данных: первый с информацией о продажах (товар, количество) и второй с инфомацией о цене товаров (товар, цена). Необходимо объединить данные и найти общую выручку от продаж каждого товара."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 2928,
     "status": "ok",
     "timestamp": 1712060401139,
     "user": {
      "displayName": "Дмитрий Морданов",
      "userId": "02885086122102025996"
     },
     "user_tz": -180
    },
    "id": "R3ceBs9_cAId",
    "outputId": "e0ae4147-3ac2-4d68-e9a1-62bbffb42d91"
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\", \"advanced3\")\n",
    "sales_data = [(\"apple\", 10), (\"banana\", 20), (\"apple\", 15), (\"banana\", 25), (\"peach\", 15), (\"peach\", 25), (\"watermelon\", 5), (\"watermelon\", 10), ]\n",
    "price_data = [(\"apple\", 2), (\"banana\", 1.5), (\"peach\", 2.5), (\"watermelon\", 3.5),]\n",
    "sales_rdd = sc.parallelize(sales_data)\n",
    "price_rdd = sc.parallelize(price_data)\n",
    "joined_rdd = sales_rdd.join(price_rdd)\n",
    "\n",
    "revenue_rdd = joined_rdd.map(lambda x: (x[0], x[1][0] * x[1][1]))\n",
    "revenue_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1111,
     "status": "ok",
     "timestamp": 1712060402248,
     "user": {
      "displayName": "Дмитрий Морданов",
      "userId": "02885086122102025996"
     },
     "user_tz": -180
    },
    "id": "C02VojCJcww-"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6B6r2QibeoIZ"
   },
   "source": [
    "### Задание 7. поиск самого длинного слова в RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 1678,
     "status": "ok",
     "timestamp": 1712060403924,
     "user": {
      "displayName": "Дмитрий Морданов",
      "userId": "02885086122102025996"
     },
     "user_tz": -180
    },
    "id": "XednYnLoeeyV",
    "outputId": "120f69e6-4512-40ee-bd0a-c3de5011d236"
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\", \"RDD tasks1\")\n",
    "data = [\"Приложение\", \"Яблоко\", \"Спарк\", \"Путеводитель\", \"Метрополитен\", \"Анализ\", \"Солнце\", \"Питон\", \"Снег\", \"Рынок\",]\n",
    "rdd = sc.parallelize(data)\n",
    "longest_word = rdd.max(key=lambda x: len(x))\n",
    "print(\"Самое длинное слово:\", longest_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1008,
     "status": "ok",
     "timestamp": 1712060404930,
     "user": {
      "displayName": "Дмитрий Морданов",
      "userId": "02885086122102025996"
     },
     "user_tz": -180
    },
    "id": "SlzH6egCfTFF"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4Z72eYffZM4"
   },
   "source": [
    "### Задание 8. Фильтрация слов по длине в RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 1300,
     "status": "ok",
     "timestamp": 1712060406227,
     "user": {
      "displayName": "Дмитрий Морданов",
      "userId": "02885086122102025996"
     },
     "user_tz": -180
    },
    "id": "-zY8y-FvflYe",
    "outputId": "8d09a7b1-d7d3-4e42-db8d-0ef063f4698c"
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\", \"RDD tasks2\")\n",
    "data = [\"Приложение\", \"Яблоко\", \"Спарк\", \"Путеводитель\", \"Метрополитен\", \"Анализ\", \"Солнце\", \"Питон\", \"Снег\", \"Рынок\",]\n",
    "rdd = sc.parallelize(data)\n",
    "filtered_rdd = rdd.filter(lambda x: len(x) > 6)\n",
    "print(\"Слова, длиной более 6 символов:\", filtered_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1022,
     "status": "ok",
     "timestamp": 1712060407246,
     "user": {
      "displayName": "Дмитрий Морданов",
      "userId": "02885086122102025996"
     },
     "user_tz": -180
    },
    "id": "vVnqUGP-fVUy"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYhSs5cLgGLJ"
   },
   "source": [
    "### Задание 9. Подсчет колиечества уникальных слов в RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 1514,
     "status": "ok",
     "timestamp": 1712060408757,
     "user": {
      "displayName": "Дмитрий Морданов",
      "userId": "02885086122102025996"
     },
     "user_tz": -180
    },
    "id": "bjJ-vuxJgC40",
    "outputId": "0ccc8b10-390a-46fe-ce66-0ccddc53db89"
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\", \"RDD tasks3\")\n",
    "data = [\"Приложение\", \"Яблоко\", \"Спарк\", \"Путеводитель\", \"Метрополитен\",\n",
    "        \"Анализ\", \"Солнце\", \"Питон\", \"Снег\", \"Рынок\", \"Яблоко\", \"Путеводитель\", \"Анализ\",]\n",
    "rdd = sc.parallelize(data)\n",
    "unique_word_count = rdd.distinct().count()\n",
    "print(\"Количество уникальных слов:\", unique_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 989,
     "status": "ok",
     "timestamp": 1712060409743,
     "user": {
      "displayName": "Дмитрий Морданов",
      "userId": "02885086122102025996"
     },
     "user_tz": -180
    },
    "id": "HAv9IWC9gPK-"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuIXl2MOgs34"
   },
   "source": [
    "### Задание 10. Преобразование всех слов в верхний регистр."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1712060409744,
     "user": {
      "displayName": "Дмитрий Морданов",
      "userId": "02885086122102025996"
     },
     "user_tz": -180
    },
    "id": "lqea8ZubhpEi"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1712060409744,
     "user": {
      "displayName": "Дмитрий Морданов",
      "userId": "02885086122102025996"
     },
     "user_tz": -180
    },
    "id": "wQt8r3cWgr6N"
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\", \"RDD tasks4\")\n",
    "data = [\"Приложение\", \"Яблоко\", \"Спарк\", \"Путеводитель\", \"Метрополитен\", \"Анализ\", \"Солнце\", \"Питон\", \"Снег\", \"Рынок\",]\n",
    "rdd = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 1447,
     "status": "ok",
     "timestamp": 1712060411186,
     "user": {
      "displayName": "Дмитрий Морданов",
      "userId": "02885086122102025996"
     },
     "user_tz": -180
    },
    "id": "V99RXiCIh89n",
    "outputId": "8ff780c3-331e-43f1-93a5-acbd300943bb"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "upper_rdd = rdd.map(lambda x: x.upper())\n",
    "print(\"Слова в верхнем регистре:\", upper_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1259,
     "status": "ok",
     "timestamp": 1712060412436,
     "user": {
      "displayName": "Дмитрий Морданов",
      "userId": "02885086122102025996"
     },
     "user_tz": -180
    },
    "id": "FaYJCMX3g57A"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0IawynUzhTaX"
   },
   "source": [
    "### Задание 11. Найти средний возраст пользователей по их покупкам и вывести топ-5 самых молодых"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 2659,
     "status": "ok",
     "timestamp": 1712060415092,
     "user": {
      "displayName": "Дмитрий Морданов",
      "userId": "02885086122102025996"
     },
     "user_tz": -180
    },
    "id": "YyX6ZotxhO0k",
    "outputId": "c791220b-99e1-449f-bf10-0a66d6410e31"
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\", \"User Age\")\n",
    "user_purchase_rdd = sc.parallelize([(1, 25), (2, 30), (3, 20), (4, 35), (5, 28), (6, 22)])\n",
    "user_age_total = user_purchase_rdd.mapValues(lambda age: (age, 1)).reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1])).mapValues(lambda v: v[0] / v[1])\n",
    "youngest_users = user_age_total.sortBy(lambda x: x[1]).take(5)\n",
    "for user_id, avg_age in youngest_users:\n",
    "  print(user_id, avg_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 989,
     "status": "ok",
     "timestamp": 1712060416078,
     "user": {
      "displayName": "Дмитрий Морданов",
      "userId": "02885086122102025996"
     },
     "user_tz": -180
    },
    "id": "8n8e4sIhjVrc"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqJeFoeFjshr"
   },
   "source": [
    "### Задание 12. Найти среднюю цену товара в каждой категории и вывести результат в формате \"Категория: Средняя цена\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 1437,
     "status": "ok",
     "timestamp": 1712060417513,
     "user": {
      "displayName": "Дмитрий Морданов",
      "userId": "02885086122102025996"
     },
     "user_tz": -180
    },
    "id": "sVu-w88BjXJb",
    "outputId": "bb2e2d39-83ff-4ff1-893f-33186b75c78e"
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\", \"Product price\")\n",
    "product_rdd = sc.parallelize([(1, \"A\", 100), (2, \"B\", 150), (3, \"A\", 120), (4, \"C\", 200), (5, \"B\", 130), (6, \"C\", 140), ])\n",
    "category_total_price = product_rdd.map(lambda x: (x[1], (x[2], 1))).reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1])).mapValues(lambda v: v[0] / v[1])\n",
    "for category, avg_price in category_total_price.collect():\n",
    "  print(category, avg_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1034,
     "status": "ok",
     "timestamp": 1712060418545,
     "user": {
      "displayName": "Дмитрий Морданов",
      "userId": "02885086122102025996"
     },
     "user_tz": -180
    },
    "id": "AoERDLJpkPkV"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4XkI3oGmi0n"
   },
   "source": [
    "### Задание 13. Найти все пары чисел из RDD, сумма которых превышает 100, и вывести их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 1335,
     "status": "ok",
     "timestamp": 1712060419877,
     "user": {
      "displayName": "Дмитрий Морданов",
      "userId": "02885086122102025996"
     },
     "user_tz": -180
    },
    "id": "i_qO0MNimQrD",
    "outputId": "154ca127-a0d8-4640-9d3d-97a20575c89e"
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\", \"NumberPairs\")\n",
    "number_rdd = sc.parallelize([30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140])\n",
    "number_pairs = number_rdd.cartesian(number_rdd).filter(lambda x: x[0] + x[1] > 100).collect()\n",
    "for pair in number_pairs:\n",
    "  print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 995,
     "status": "ok",
     "timestamp": 1712060420870,
     "user": {
      "displayName": "Дмитрий Морданов",
      "userId": "02885086122102025996"
     },
     "user_tz": -180
    },
    "id": "W2qPwt5smvn1"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XY4Dy4-LoGLc"
   },
   "source": [
    "### Бонус"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8dBA8gxqKdL"
   },
   "source": [
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Создание основной сессии Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Создание дополнительной сессии Spark с другой конфигурацией\n",
    "spark2 = spark.newSession() \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .getOrCreate()\n",
    "# Теперь у вас есть две сессии Spark: spark и spark2 с разными конфигурациями\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Hg-6JcOPvP3"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taW3JhkzoKAm"
   },
   "source": [
    "<a name=\"dz1\"></a>\n",
    "## Домашнее задание\n",
    "### Найти самую длинную последовательность упорядоченных чисел в RDD и вывести ее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 2634,
     "status": "ok",
     "timestamp": 1712060423501,
     "user": {
      "displayName": "Дмитрий Морданов",
      "userId": "02885086122102025996"
     },
     "user_tz": -180
    },
    "id": "T7Gm0qUhbWQD",
    "outputId": "e2e3fd3f-1b2a-46b8-8ccf-824b9bcf9c6e"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(20)\n",
    "\n",
    "data = [random.randint(0,10) for _ in range(20)]\n",
    "\n",
    "sc = SparkContext(\"local\", \"LongestSequence\")\n",
    "\n",
    "# Создаем RDD из data\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Function to identify sequences of ordered numbers\n",
    "# Функция генерирует последовательности упорядоченных чисел и возвращает их\n",
    "def find_sequences(iter):\n",
    "    current_seq = []\n",
    "    for num in iter:\n",
    "        if not current_seq:               # первая итерация current_seq - пустая\n",
    "            current_seq.append(num)\n",
    "        elif num >= current_seq[-1] + 1:  # текущая последовательность\n",
    "            current_seq.append(num)\n",
    "        else:                             # последовательность кончилась\n",
    "            yield current_seq             # начинаем новую\n",
    "            current_seq = [num]\n",
    "    yield current_seq\n",
    "\n",
    "print(\"Исходные данные:\", rdd.collect())\n",
    "# Ищем последовательность упорядоченных чисел в RDD\n",
    "sequences_rdd = rdd.mapPartitions(find_sequences)\n",
    "\n",
    "# Определяем наидлиннейшую последовательность\n",
    "longest_sequence = sequences_rdd.reduce(lambda x, y:\n",
    "                                        x if len(x) > len(y) else y)\n",
    "\n",
    "print(\"Самая длинная последовательность отсортированных чисел в списке:\",\n",
    "       longest_sequence)\n",
    "\n",
    "\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNiHAjkSRA0Otml2S1WrG4z",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
